<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Intro to R for Data Analysis</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">
   <!-- <link rel="stylesheet" href="css/reset.css"> -->


    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">
<!--      <header style="position: absolute;top: 50px; left: 100px; z-index:500; font-size:100px;background-color: rgba(0,0,0,0.5)"></header>
-->
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
	<section data-background="img/trees.jpg" style="background: rgba(0,129,195,.9); color: white">
	  <h1>R</h1>
	  <h3>Intro to Data Analysis and Visualization</h3>
	  <p>
	    <small>Greg Heartsfield, 2014</small>
	  </p>
	</section>
        <section data-background="img/arch.jpg" style="background: rgba(50,50,50,.5); color: white">
          <h2>Agenda, Goals &amp; AntiGoals</h2>
          <ul>
            <li>Learn enough R to be productive</li>
            <li>Learn how to learn R</li>
            <li>Understand R's strengths &amp; weaknesses</li>
            <li>Demonstrate real-world usage
              <ul>
                <li>Mostly loading and munging data</li>
                <li>A little bit of analysis/visualization</li>
              </ul>
              </li>
            <li><i>Not</i> convince you R is easy</li>
            <li><i>Not</i> teach statistics</li>
            <li><i>Not</i> show everything that R can do</li>
          </ul>
<hr>
          <p>Some broad overview of R is needed first, but the goal is to spend as much time as possible in front of an R session, working through two case studies.</p>
        </section>
        <section>
	  <section>
	    <h2>What is R</h2>
            <iframe width="820" height="500" src="//www.youtube.com/embed/TR2bHSJ_eck" frameborder="0" allowfullscreen></iframe>
	  </section>

	  <section>
	    <h2>What is R</h2>
	    <ul>
              <li>Free software environment for statistical computing, 20 years of development.</li>
              <li>Strongly influenced by the S language, created in 1976.</li>
              <li>Focused on statistics and visualization.</li>
              <li>Runs on all modern platforms.</li>
              <li>Competes strongly with high-end commercial packages like SPSS and SAS.</li>
              <li>Taught in universities, used in mission-critical applications</li>
	    </ul>
	  </section>
        </section>

        <section>
          <section>
            <h2>Why R (instead of Excel)?</h2>
            <ul>
              <li>Volume - Millions or billions of rows</li>
              <li>Automation - Script analysis and graphics creation</li>
              <li>Complexity - Flexible data cleanup, sophisticated algorithms for analysis</li>
              <li>Accuracy - <a href="http://panko.shidler.hawaii.edu/SSR/Mypapers/whatknow.htm">&quot;Spreadsheets, even after careful development, contain errors in 1% or more of all formula cells&quot;</a></li>
              <li>Auditability - Data + Script is 100% repeatable and transparent</li>
              <li>Visualization - Exploratory to publication quality graphics</li>
	    </ul>
          </section>
          <section>
            <h2>Why Excel?</h2>
            <ul>
              <li>Manual data entry</li>
              <li>One-off simple calculations</li>
              <li>You <i>really</i> need that 3-D pie chart</li>
	    </ul>
          </section>
          
	</section>

        <section>
          <h2>Installing R</h2>
          <p><a href="http://cran.rstudio.com/bin/windows/base/">Download R for Windows</a></p>
          <p><a href="http://cran.rstudio.com/bin/windows/base/">Download RStudio IDE</a> (optional, requires R)</p>
<hr>
          <p>Install packages that we'll be using</p>
          <p><pre><code class="lang-r">install.packages(c("plyr", "ggplot2", "lubridate"))</code></pre></p>
<ul>
<li><code>plyr</code> gives us powerful data manipulation tools</li>
<li><code>ggplot2</code> makes beautiful visualizations</li>
<li><code>lubridate</code> makes working with dates and time simple</li>
</ul>
        </section>

        <section>
          <section>
            <h2>The R(Studio) Environment</h2>
            <img src="img/rstudio_overview.png">
          </section>
          <section>
            <h2>Being Productive</h2>
            <ul>
              <li>Organize data, scripts, and plots in the same directory</li>
              <li>Session &raquo; Set Working Directory &raquo; To Source File Location</li>
              <li>Pick a way of working:</li>
              <ul>
                <li>explore with console, saving the good bits into a script</li>
                <li>everything in console, run savehistory() and keep the good bits at the end</li>
              </ul>
            </ul>
          </section>
        </section>

        <section>
            <h2>Experimenting with the Console</h2>
          <section>

            <p>R can add</p>
            <pre><code class="lang-r">&gt; 1.9+1
[1] 2.9</code></pre>
          </section>

          <section>
            <p>R can add vectors...</p>
            <pre><code class="lang-r">&gt; c(1,10) + c(2,20)
[1]  3 30</code></pre>

            <p>store and recall variables,</p>
            <pre><code class="lang-r">&gt; odds <- seq(from=1, to=30, by=2)
&gt; odds
 [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29</code></pre>
            <p>and summarize them.</p>
            <pre><code class="lang-r">&gt; summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1       8      15      15      22      29</code></pre>
          </section>

          <section>
            <p>Variables can be listed and deleted</p>
            <pre><code class="lang-r">&gt; ls()
[1] "odds"
&gt; rm(odds)</code></pre>

            <p>Misc. other commands</p>
            <pre><code class="no-highlight">&gt; history()    # list all commands entered
&gt; ?name        # show online help for package or function "name"
&gt; ??name       # search online help for anything containing "name"
&gt; help.start() # browse a local copy of the R reference docs</code></pre>
          </section>

        </section>

        <section>
          <section>
            <h2>Loading Data</h2>
            <p>R has dozens of ways to import your data.  Text, databases, binary files, Excel, network services, etc.</p>
            <ul>
            <li><code>read.csv()</code> is what you need 99% of the time for small comma-delimited text (~500MB)</li>
            <li><code>read.table()</code> has better defaults for space-delimited text.</li>
            <li><code>read.csv.sql()</code> reads data into SQLite, then uses an SQL select query to bring a portion into R. (fast)</li>
            <li><code>fread()</code> in the <code>data.table</code> package for multi-GB files. (fastest)</li>
            </ul>
          </section>

          <section>
            <h2>Loading CSV (simple)</h2>
            <p><pre><code class="no-highlight">first,last
max,theiler
john,enders</code></pre></p>
            <p><pre><code class="lang-r">&gt; read.csv("names.csv")              </code></pre></p>
            <p><pre><code class="no-highlight">  first    last
1   max theiler
2  john  enders</code></pre></p>
          </section>
          <section>
            <h2>Loading CSV, pt2</h2>
            <p><pre><code class="no-highlight">Some uncommented junk in the header
max  $  theiler $ 1899$US$"Delimiter $ in quotes"
# just a comment followed by a blank line

john $  enders  $ 1897$ZA$</code></pre></p>
            <p><pre><code class="lang-r">&gt; read.csv("names2.csv",
         header=FALSE, sep="$",
         skip=1, strip.white=TRUE,
         comment.char="#",
         col.names=c("first","last","birth.year",
                     "birth.country","comment"))</code></pre></p>
            <p><pre><code class="no-highlight">  first    last birth.year birth.country               comment
1   max theiler       1899            US Delimiter $ in quotes
2  john  enders       1897            ZA</code></pre></p>
          </section>

          <section>
            <h2>Loading CSV, pt3</h2>
            <p>Override default types</p>
            <p><pre><code class="no-highlight">userid,name,age,phone
100,max,45,555-0000
101,john,22,555-2222</code></pre></p>
            <p><pre><code class="lang-r">&gt; x <- read.csv("names3.csv", 
                colClasses=c("character", "factor", "numeric", "character"))</code></pre></p>
            <p>Check the structure</p>
            <p><pre><code class="lang-r">&gt; str(x)
'data.frame':	2 obs. of  4 variables:
 $ userid: chr  "100" "101"
 $ name  : Factor w/ 2 levels "john","max": 2 1
 $ age   : num  45 22
 $ phone : chr  "555-0000" "555-2222"</code></pre></p>
          </section>

        </section>

        <section>
          <h2>Data Types</h2>
          <section>
            <h3>Atomic Types</h3>
            <table>
              <tr><th>Type</th><th>Representation</th><th>Examples</th></tr>
              <tr><td>integer</td> <td>32-bit long (warns on overflow)</td> <td>1382504927</td></tr>
              <tr><td>numeric</td> <td>64-bit double, floating point</td> <td>4.527471e+30</td></tr>
              <tr><td>character</td> <td>UTF-8 supported, auto-caching</td> <td>"this is a string"</td></tr>
              <tr><td>logical</td> <td>32-bit</td> <td><code>TRUE</code> and <code>FALSE</code></td></tr>
            </table>
          </section>
          <section>
            <h3>Data Objects</h3>
            <ul>
              <li>vector: sequence of values of the same type</li>
              <ul><li><pre><code class="lang-r">c(1,2,3,4)</code></pre></li></ul>
              <li>list: collection of values of different types</li>
              <ul><li><pre><code class="lang-r">list("one", "two", 3, 4.0)</code></pre></li></ul>
              <li>factor: mapping from names to numbers</li>
              <ul><li><pre><code class="lang-r">factor("TX", "CA", "NY")</code></pre></li></ul>
              <li>data.frame: similar to a relational database table</li>
              <ul><li><pre><code class="lang-r">data.frame(state=c("TX", "CA"), pop.mil=c(25, 39))</code></pre></li></ul>
            </ul>
          </section>
        </section>

        <section>
          <section>
          <h2>Case Study 1 (Using Data Frames)</h2>
          <blockquote>The UCSB International Capture The Flag (also known as the iCTF) is a distributed, wide-area security exercise, whose goal is to test the security skills of the participants.</blockquote>
          <p>The 2002 contest includes system logs from machines under attack, we'll use the HTTP logs here to investigate loading and analyzing real-world data, primarily to understand how to work with dataframes.</p>
          </section>
          <section>
            <h2>First Look</h2>
            <p>If you suspect delimited data, start with <code>read.csv</code> and peek at the first few rows.</p>
            <p><pre><code class="lang-r">s <-read.csv("treasurehunt_alpha_httpd_access.log",nrows=5)</code></pre></p>
            <p>Use <code>View(s)</code> to see the raw data.  Datatypes are shown with <code>str(s)</code>.</p>
            <p>Issues noted: no header, space delimited, "-" for blanks, missing fields...</p>
          </section>
          <section>
            <h2>Reading Full Dataset</h2>
            <p><pre><code class="lang-r">s <-read.table("treasurehunt_alpha_httpd_access.log",
               fill=TRUE,       # missing fields set as blank
               na.strings = "-",         # "-" set to NA
               stringsAsFactors = FALSE) # do not use factors</code></pre></p>
            <p>Get a better look at identified columns, and assign names<p>
            <p><pre><code class="lang-r">colnames(s) <- c("client", "identd", "userid",
                 "date.1", "date.2", "request",
                 "status", "size",
                 "referrer", "useragent")</code></pre></p>
          </section>
          <section>
            <h2>Discarding Data</h2>
            <p>If we have data we know will not be used, go ahead and discard it early on.</p>
            <p><pre><code class="lang-r">> summary(s$identd)
   Mode    NA's 
logical    2522</code></pre></p>
            <p>Set a check so that the script will error out if we ever encounter values here.  Then, we can confidently delete it.</p>
            <p><pre><code class="lang-r">stopifnot(is.na(s$identd))</code></pre></p>
            <p><pre><code class="lang-r">s$identd <- NULL</code></pre></p>
          </section>
          <section>
            <h2>Parsing Dates</h2>
            <p><pre><code class="lang-r"># Combine date components
s$req.date <- apply(s[,c("date.1","date.2")], 1, paste, collapse=" ")
# Parse these as real dates:
s$req.date <- as.POSIXct(strptime(s$req.date, "[%d/%b/%Y:%T %z]"))
# Delete the old date columns
s[,c("date.1","date.2")] <- list(NULL)</code></pre></p>
          </section>
          <section>
            <h2>Categorical Variables</h2>
            <p>HTTP status looks like a number, but we should not treat it like one.  Why?  Because taking the average makes no sense.  Reassign it as a categorical variable (a factor).
            <p><pre><code class="lang-r">s$status <- factor(s$status)</code></pre></p>
          </section>
          <section>
            <h2>Rewriting Variables</h2>
            <p>When no data was sent, this was recorded as a blank or <code>NA</code> value.  We know this really means "0 bytes sent", so we can rewrite the column.</p>
            <p><code>with()</code> lets us refer to a dataframes variables with the <code>df$var</code> syntax.</p>
            <p><code>ifelse()</code> runs a test on a vector, and replaces values based on whether the test was true.</p>
            <p><pre><code class="lang-r">s$size <- with(s,
                 ifelse(is.na(size),
                        yes=0,
                        no=size))</code></pre></p>
          </section>
          <section>
            <h2>Merging Datasets (prep)</h2>
            <p>Often you will need to incorporate multiple datasets together for an analysis.</p>
            <p>Lets read in another dataset, giving us GeoIP data for the IP's in the web logs.</p>
            <p><pre><code class="lang-r">geoip <- read.csv("treasure_hunt_geoip.csv",
                  na.strings="", stringsAsFactors=FALSE)</code></pre></p>
            <p><pre><code class="lang-r">&gt; str(geoip)
'data.frame':	40 obs. of  8 variables:
 $ IP          : chr  "128.111.48.107" ...
 $ host        : chr  "fleming.cs.ucsb.edu" ...
 $ country.code: chr  "US" ...
 $ city        : chr  "Santa Barbara" ...
 $ postal.code : int  93106 10469 ...
 $ longitude   : num  -120 ...
 $ latitude    : num  34.4 ...
 $ match       : chr  "fleming.cs.ucsb.edu" ...</code></pre></p>
          </section>
          <section>
            <h2>Merging Datasets (prep)</h2>
            <p>One problem... our original dataset resolved some, but not all client addresses</p>
            <p><pre><code class="lang-r">geoip <- rbind(geoip,geoip) # duplicate all rows
geoip$match <- geoip$IP # make a new "match" var from IP
half <- 1:nrow(geoip)/2 # a sequence of half our rows
# overwrite the IP with Host for half the rows
geoip[half,"match"] <- geoip[half,"host"]</code></pre></p>
          </section>
          <section>
            <h2>Merging Datasets</h2>
            <p>We're finally ready to merge.  We'll use the equivalent of a left outer join.</p>
            <p><pre><code class="lang-r">s.loc &lt;- merge(s,     # left "table"
               geoip, # right "table"
               by.x="client", # match this var
               by.y="match",  # against this
               all.x=TRUE)    # don't drop rows from s</code></pre></p>
            <p>Now, <code>s.loc</code> contains all the requests, along with location data.</p>
            <p><pre><code class="lang-r">&gt; s.loc[50,c("client", "status", "city")]
          client status          city
50 128.111.43.25    304 Santa Barbara</code></pre></p>
          </section>
          <section>
            <h2>Tabulating</h2>
            <p>Our first simple analysis... lets put our GeoIP data to work and understand how cities and HTTP status codes relate:</p>
            <p><pre><code class="lang-r">> table(s.loc$status, s.loc$city, useNA="ifany")
     
      Bronx King Of Prussia Santa Barbara &lt;NA&gt;
  200     1               1           742    7
  301     0               0             2    0
  304     0               0            37    6
  400     0               0            33    1
  403     0               0            11    0
  404     0               0          1668    0
  405     0               0             2    0
  408     0               0             2    0
  501     0               0             9    0</code></pre></p>
          </section>
        </section>

	<section>
          <section>
	    <h2>Case Study 1A S-A-C</h2>
	    <p>Now we have clean web logs, joined with some potentially useful location data.  We'll use a common technique for cleaning &amp; analysing data called "split-apply-combine" to do further analysis.</p>
            <ul>
              <li><b>Split:</b> create groups based on some common attribute.</li>
              <li><b>Apply:</b> transform each group using a function (such as sum or average).</li>
              <li><b>Combine:</b> bring the transformed groups back together in one structure.</li>
            </ul>
	  </section>
          <section>
	    <h2>Split-Apply-Combine Illust.</h2>
            <img src="img/sac.png" width="60%" height="60%"/>
          </section>

          <section>
	    <h2>Simple Example</h2>
            <p><pre><code class="lang-r">ddply(s.loc, # dataset
      .(status), # Split on HTTP status
      summarize, # Function to Apply
        max.response = max(size)) # Arguments</code></pre></p>
            <p><pre><code class="lang-r">  status max.response
1    200        44192
2    301          293
3    304            0
4    400          514
5    403          298
6    404          386
7    405          310
8    408            0
9    501          332</code></pre></p>
          </section>

          <section>
	    <h2>Another Example</h2>
            <p><pre><code class="lang-r">ddply(s.loc, .(country.code, status), summarize,
      size.total=sum(size, na.rm=TRUE), 
      size.mean=mean(size, na.rm=TRUE), 
      request.count=length(size))</code></pre></p>
            <p><pre><code class="lang-r">   country.code status size.total size.mean request.count
1            TW    400        318  318.0000             1
2            US    200    1151179 1547.2836           744
3            US    301        585  292.5000             2
4            US    304          0    0.0000            37
5            US    400      11587  351.1212            33
6            US    403       2844  258.5455            11
7            US    404     474735  284.6133          1668
8            US    405        618  309.0000             2
9            US    408          0    0.0000             2
10           US    501        978  108.6667             9
11         &lt;NA&gt;    200      30082 4297.4286             7
12         &lt;NA&gt;    304          0    0.0000             6</code></pre></p>
          </section>

        </section>
	<section>
          <section>
	    <h2>Case Study 2 (PCAP)</h2>
	    <p>The 2nd dataset, also from UCSB iCTF, is 2.3GB of network captures (PCAP) taken during a capture-the-flag contest.</p>
            <p><a href="http://ictf.cs.ucsb.edu/archive/iCTF_2003/setup.html">A network topology is provided.</a></p>
            <img src="img/ctf-topology.png" width="60%" height="60%"/>
	  </section>
          <section>
            <h2>Parsing PCAP</h2>
            <p>R has no special tools for parsing PCAP, wireshark and snort are better tools for parsing network packets.  Here we extracted some basic IP, TCP, and HTTP attributes from the captures, resulting in about 6.9 million rows of tab-separated data.</p>
            <p><pre><code class="lang-r">tshark -r ictf.pcap -T fields -e frame.time -e ip.src
  -e ip.dst -e ip.proto -e tcp.srcport
  -e tcp.dstport -e http.request.method
  -e http.request.uri -e http.host 
  -e http.authorization -e http.connection
  -e http.cookie -e http.response.code
  -e http.user_agent -E header=y
  -E separator=/t -E quote=n
  -E occurrence=f &gt; ictf2003.tsv</code></pre></p>
          </section>

          <section>
            <h2>Loading in R</h2>
            <p>Data is easily readable in R, with some important additional flags to <code>read.table()</code>, and timestamp parsing.</p>
            <p><pre><code class="lang-r">i &lt;- read.table("ictf2003.tsv",
                sep="\t",
                header=TRUE,
                na.strings="",
                fill=TRUE,
                nrows=1000000, # improve speed for testing
                comment="", # appears in strings
                colClasses=c("character", 
                             rep("factor", 6), 
                             "character",
                             rep("factor", 6)))
i$frame.time <- strptime(i$frame.time, "%b  %e, %Y %H:%M:%OS")</code></pre></p>
                </section>
          <section>
            <h2>Line Charts</h2>
            <p>The obvious first thing to look at is traffic by protocol.  Let us summarize packets/minute for each protocol on a plot.</p>
            <p>Prep the data by creating a new variable for the timestamp, at 1-minute resolution:</p>
            <p><pre><code class="lang-r">i$frame.minute <- round_date(i$frame.time, "minute")</code></pre></p>
            <p>Group by the minute and protocol, and find how many packets exist for each grouping:</p>
            <p><pre><code class="lang-r">i.ppm &lt;- ddply(i, .(frame.minute, ip.proto), 
               summarise, packets=length(ip.proto))</code></pre></p>
            <p>Line plot using our new variables, color by protocol:</p>
            <p><pre><code class="lang-r">qplot(data=i.ppm, y=packets, x=frame.minute, geom="line", colour=ip.proto)</code></pre></p>
                                                                                                                              </section>
          <section>
            <h2>Line Charts</h2>
            <img src="img/plot_packets_per_minute_by_proto.png" width="100%" height="100%"/>
          </section>
          <section>
            <h2>Trellis/Faceted Plots</h2>
            <p>Facets, or "trellis" plots, are very useful with complex datasets that would otherwise have too much overlapping data to be legible.  Adding "jitter" to the data can also help when overlap would obscure data.</p>
            <img src="img/plot_http.methods.top.5.src.ips.png" width="70%" height="70%"/>
          </section>
          <section>
            <h2>Trellis Contd</h2>
            <p>That's a really complicated plot to make, right? It's not too bad...<p>
<p>A bit of munging needed to find the top-5 source IPs:</p>
<p><pre><code class="lang-r">src.ip.counts <- as.data.frame(table(i.http.method$ip.src))
top5.ips &lt;- head(src.ip.counts[order(-src.ip.counts$Freq),1],n=5)
i.top5 &lt;- i.http.method[i.http.method$ip.src %in% top5.ips,]</code></pre></p>
<p>The plot itself is very simple:</p>
            <p><pre><code class="lang-r">qplot(frame.time,  # X-axis, time
      ip.src,      # Y-axis, source IP
      data=i.top5, # Dataset
      colour=http.request.method, # What to color
      geom="jitter" # How to draw data, "jittered points"
      facets=~http.request.method) # What to repeat in each facet</code></pre></p>

          </section>
        </section>
<!--
	<section data-background="img/the_end.jpg" style="background: rgba(0,129,195,0); color: white">
          <h2>Thank You!</h2>
	</section>
-->
      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
      controls: true,
      progress: true,
      history: true,
      center: false,

      theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
      transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

      // Parallax scrolling
      // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
      // parallaxBackgroundSize: '2100px 900px',

      // Optional libraries used to extend on reveal.js
      dependencies: [
      { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
      ]
      });

    </script>

  </body>
</html>
